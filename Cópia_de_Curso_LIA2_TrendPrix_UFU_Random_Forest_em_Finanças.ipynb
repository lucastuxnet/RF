{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exemplos e Conceitos"
      ],
      "metadata": {
        "id": "lkzVs_zf8ykr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo geeksforgeeks (nao concluido)\n",
        "\n"
      ],
      "metadata": {
        "id": "r1uMkGg149uM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extraído de https://www.geeksforgeeks.org/random-forest-regression-in-python/\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('Salaries.csv')\n",
        "print(data)\n",
        "\n",
        "\n",
        "x = df.iloc[:, : -1]\n",
        "y = df.iloc[:, -1:]\n",
        "\n",
        "# Fitting Random Forest Regression to the dataset\n",
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# create regressor object\n",
        "regressor = RandomForestRegressor(n_estimators=100,\n",
        "                                  random_state=0)\n",
        "\n",
        "# fit the regressor with x and y data\n",
        "regressor.fit(x, y)\n",
        "\n",
        "Y_pred = regressor.predict(np.array([6.5]).reshape(1, 1))\n",
        "\n",
        "# Visualising the Random Forest Regression results\n",
        "\n",
        "# arrange for creating a range of values\n",
        "# from min value of x to max\n",
        "# value of x with a difference of 0.01\n",
        "# between two consecutive values\n",
        "X_grid = np.arrange(min(x), max(x), 0.01)\n",
        "\n",
        "# reshape for reshaping the data\n",
        "# into a len(X_grid)*1 array,\n",
        "# i.e. to make a column out of the X_grid value\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "\n",
        "# Scatter plot for original data\n",
        "plt.scatter(x, y, color='blue')\n",
        "\n",
        "# plot predicted data\n",
        "plt.plot(X_grid, regressor.predict(X_grid),\n",
        "\t\tcolor='green')\n",
        "plt.title('Random Forest Regression')\n",
        "plt.xlabel('Position level')\n",
        "plt.ylabel('Salary')\n",
        "plt.show()\n",
        "\n",
        "from sklearn.trees import RandomForestClassifier\n",
        "RandomeForest = RandomForestClassifier(oob_score=True)\n",
        "RandomForest.fit(X_train,y_train)\n",
        "print(RandomForest.oob_score_)"
      ],
      "metadata": {
        "id": "eF_5gjAq4DqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo Medium"
      ],
      "metadata": {
        "id": "IMV7bPb74Eh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraído de https://medium.com/@theclickreader/random-forest-regression-explained-with-implementation-in-python-3dad88caf165\n",
        "# Importing the libraries\n",
        "import numpy as np # for array operations\n",
        "import pandas as pd # for working with DataFrames\n",
        "import requests, io # for HTTP requests and I/O commands\n",
        "import matplotlib.pyplot as plt # for data visualization\n",
        "%matplotlib inline\n",
        "\n",
        "# scikit-learn modules\n",
        "from sklearn.model_selection import train_test_split # for splitting the data\n",
        "from sklearn.metrics import mean_squared_error # for calculating the cost function\n",
        "from sklearn.ensemble import RandomForestRegressor # for building the model\n",
        "\n",
        "# Importing the dataset from the url of the data set\n",
        "url = \"https://drive.google.com/u/0/uc?id=1mVmGNx6cbfvRHC_DvF12ZL3wGLSHD9f_&export=download\"\n",
        "data = requests.get(url).content\n",
        "\n",
        "# Reading the data\n",
        "dataset = pd.read_csv(io.StringIO(data.decode('utf-8')))\n",
        "dataset.head()\n",
        "\n",
        "x = dataset.drop('Petrol_Consumption', axis = 1) # Features\n",
        "y = dataset['Petrol_Consumption']  # Target\n",
        "\n",
        "# Splitting the dataset into training and testing set (80/20)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 28)\n",
        "\n",
        "# Initializing the Random Forest Regression model with 10 decision trees\n",
        "model = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
        "\n",
        "# Fitting the Random Forest Regression model to the data\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# Predicting the target values of the test set\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# RMSE (Root Mean Square Error)\n",
        "rmse = float(format(np.sqrt(mean_squared_error(y_test, y_pred)),'.3f'))\n",
        "print(\"\\nRMSE:\\n\",rmse)"
      ],
      "metadata": {
        "id": "IltlhBjS4Cn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo ComputerScience (adaptado)"
      ],
      "metadata": {
        "id": "sk6gOkW24L73"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnsvo03JBBs"
      },
      "source": [
        "### Predicting Stock Movement with Random Forest\n",
        "\n",
        "GitHub For this Paper can be found here: https://github.com/LuckysonKhaidem/StockMarketPedicition\n",
        "\n",
        "Random Forest is a machine learning ensemble method that is widely used because of it's flexibility, simplicity, and often quality results. In this tutorial, we will use the Random Forest algorithim to build a classification model that will help us predict whether a stock will close up or down based on range of technical indicators.\n",
        "\n",
        "## What is Random Forest?\n",
        "Now, up above I gave you a pretty simple definition of what Random Forest is. However, I think it's important to delve in more detail about it before we start coding our model. Now in short, Random Forests is a supervised machine learning algorithim that uses multiple decision trees in aggregate to help make more stable and accurate predictions.\n",
        "\n",
        "After reading that you're probably going, \"Well you answered one question but left me with a few more.\" Let's go over some of those questions.\n",
        "\n",
        "### What are Decision Trees?\n",
        "Decision Trees are the fundamental building blocks of Random Forest. In essence, Decision Trees is flowlike chart structure where each node of the tree is used to test a particular attribute of the object. For example, imagine I have a person which will represent our object. We then test certain attributes of this person object. For example, one test would be whether they are male or female. The test will represent a \"Decision Node\" in our tree, and each of the possible outcomes \"Male\" or \"Female\" will represent a leaf node.The first \"Decision Node\" in our Decision Tree will be our \"Root Node\"\n",
        "\n",
        "- **Root Node:** Represents entire population or sample and this further gets divided into two or more homogeneous sets. Our starting point.\n",
        "- **Splitting:** The process of dividing a node into two or more sub-nodes, for example we split on gender.\n",
        "- **Decision Node:** When a sub-node splits into further sub-nodes, then it is called decision node.\n",
        "- **Leaf/Terminal Node:** Nodes do not split is called Leaf or Terminal node.\n",
        "- **Pruning:** When we remove sub-nodes of a decision node, this process is called pruning. You can say opposite process of splitting.\n",
        "- **Branch/Sub-Tree:** A sub section of entire tree is called branch or sub-tree.\n",
        "- **Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes whereas sub-nodes are the child of parent node.\n",
        "\n",
        "### What is Ensemble Learning?\n",
        "An Ensemble Learning model is a model in which decisions are used from MULTIPLE MODELS to improve the overall performance of the model. The old idea that 2 minds are better than 1, prefectly summarizes ensemble learning. We use the results of multiple models to get a better idea of what the true answer is. Our Random Forest algorithim uses Bagging to help improve performance.\n",
        "\n",
        "### Why use Random Forest?\n",
        "Now, when you read the definition about decision trees you may be thinking, \"That sounds like a really simple and intuitive model. Why would we not just use decision trees?\". Well, it boils down to the following few points:\n",
        "\n",
        "1. Instability: Even small changes to the input data can have dramatic changes to the overall strucutre of the decision tree.\n",
        "2. They are often relatively inaccurate. Many other predictors perform better with similar data.\n",
        "3. For data including categorical variables with different number of levels, information gain in decision trees is biased in favor of those attributes with more levels.\n",
        "4. Calculations can get very complex, particularly if many values are uncertain and/or if many outcomes are linked.\n",
        "\n",
        "These are some of the reasons it's preferable to use Random Forest because we will see that it helps overcome some of the weaknesses of Decision Trees. Now, as with anything, there is no perfect model. Just because something has weaknesses means it's worthless it just means we have to understand those weaknesses and keep eye out for them as we use them.\n",
        "\n",
        "### What is Supervised Learning?\n",
        "In machine learning, we have two categories of learning. Supervised learning and unsupervised learning. With unsupervised learning, we don't supervise the model and instead allow it to discover information on it's own. We do this by providing an \"UNLABELED\" data set that doesn't tell the model what category or value is the \"correct\" answer.\n",
        "\n",
        "With supervised learning, we provide the model with a \"LABELED\" data set which tells the model what the \"correct\" value it should be. Random Forest, is an example of a supervised learning algorithim because we provide the model a labeled data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYK8sDelJBBt"
      },
      "outputs": [],
      "source": [
        "#OBS: editado quando necessário. Veja mudanças marcadas com ###\n",
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import RocCurveDisplay ### 1 antes era plot_roc_curve\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "#from config import ACCOUNT_NUMBER, ACCOUNT_PASSWORD, CONSUMER_ID, REDIRECT_URI ###2\n",
        "#pip install --upgrade scikit-learn #if any sklearn tool get problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgumNSGJBBu"
      },
      "source": [
        "### Data Preprocessing: Loading the TD API Library\n",
        "\n",
        "For those of you who watched my series on building a Python API Client library for TD Ameritrade, here is a chance to put that library to use. I want to collect some price data on a few stocks. Let's use the TD Library to do that. First, because it's not an installed library on my system, I'll need to add the location of the library to my system path.\n",
        "\n",
        "Define the path to the folder, use the `sys.path.insert` method to insert that path and then load the library as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mGE1YYLJBBu"
      },
      "outputs": [],
      "source": [
        "### 3\n",
        "# Define path to the TD API folder.\n",
        "#path_to_td_folder = r\"PATH_TO_TD_LIBRARY\"\n",
        "\n",
        "# I'll be needing my TD API Client to get some prices, so I'll need to point a path to it.\n",
        "#sys.path.insert(0, path_to_td_folder)\n",
        "\n",
        "# import the TDClient, may get an Intellisense error but disregard it.\n",
        "#from td.client import TDClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGmG12KhJVhl"
      },
      "outputs": [],
      "source": [
        "### 4\n",
        "import yfinance as yf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Li-gDJBJBBu"
      },
      "source": [
        "### Data Preprocessign: Grabbing Historical Price Data\n",
        "\n",
        "I have a few stocks I would like to grab historical daily prices for. What I'll do is create a function that will log me into a new TD API Session, loop through a list of ticker symbols, and grab some historical prices using the `Get_Prices` endpoint. After I have my prices, I'll parse the JSON string that's returned so we can get the candle data, store it in.\n",
        "\n",
        "I'll take the final value and store it in a  `Pandas Data Frame` and then save that data frame to a CSV file so we can manually explore the data if need be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKgvVYPPJBBv"
      },
      "outputs": [],
      "source": [
        "### 4\n",
        "#def grab_price_data():\n",
        "\n",
        "    # Create a new session\n",
        "    TDSession = TDClient(account_number = ACCOUNT_NUMBER,\n",
        "                         account_password = ACCOUNT_PASSWORD,\n",
        "                         consumer_id = CONSUMER_ID,\n",
        "                         redirect_uri = REDIRECT_URI)\n",
        "\n",
        "    # Login to the session\n",
        "    TDSession.login()\n",
        "\n",
        "    # Let's define some tickers we want to get the data for five tickers.\n",
        "\n",
        "    '''\n",
        "        HD   - Home Depot\n",
        "        JPM  - JPMorgan Chase & Co.\n",
        "        IBM  - International Business Machines Corporation\n",
        "        ARWR - Arrowhead Pharmaceuticals, Inc.\n",
        "        COST - Costco Wholesale Corporation\n",
        "    '''\n",
        "\n",
        "    # Define the list of tickers\n",
        "    tickers_list = ['JPM', 'COST', 'IBM', 'HD', 'ARWR']\n",
        "\n",
        "    # I need to store multiple result sets.\n",
        "    full_price_history = []\n",
        "\n",
        "    for ticker in tickers_list:\n",
        "\n",
        "        # Grab the daily price history for 1 year\n",
        "        price_history = TDSession.get_price_history(symbol = ticker,\n",
        "                                                    periodType = 'year',\n",
        "                                                    period = 2,\n",
        "                                                    frequency = 1,\n",
        "                                                    frequencyType = 'daily',\n",
        "                                                    needExtendedHoursData = False)\n",
        "\n",
        "        # grab just the candles, and add them to the list.\n",
        "        for candle in price_history['candles']:\n",
        "            candle['symbol'] = price_history['symbol']\n",
        "            full_price_history.append(candle)\n",
        "\n",
        "    # dump the data to a CSV file, don't have an index column\n",
        "    price_data = pd.DataFrame(full_price_history).to_csv('price_data.csv', index_label = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atnZ7hwAKlv_"
      },
      "outputs": [],
      "source": [
        "# Let's define some tickers we want to get the data for five tickers.\n",
        "'''\n",
        "        HD   - Home Depot\n",
        "        JPM  - JPMorgan Chase & Co.\n",
        "        IBM  - International Business Machines Corporation\n",
        "        ARWR - Arrowhead Pharmaceuticals, Inc.\n",
        "        COST - Costco Wholesale Corporation\n",
        "'''\n",
        "\n",
        "# Define the list of tickers: 'JPM', 'COST', 'IBM', 'HD', 'ARWR'\n",
        "JPM = yf.download('JPM', start='2000-01-01', end='2022-12-31')\n",
        "JPM = JPM.drop('Adj Close',axis=1); JPM['symbol'] = 'JPM'\n",
        "COST = yf.download('COST', start='2000-01-01', end='2022-12-31')\n",
        "COST = COST.drop('Adj Close',axis=1); COST['symbol'] = 'COST'\n",
        "IBM = yf.download('IBM', start='2000-01-01', end='2022-12-31')\n",
        "IBM = IBM.drop('Adj Close',axis=1); IBM['symbol'] = 'IBM'\n",
        "HD = yf.download('HD', start='2000-01-01', end='2022-12-31')\n",
        "HD = HD.drop('Adj Close',axis=1); HD['symbol'] = 'HD'\n",
        "ARWR = yf.download('ARWR', start='2000-01-01', end='2022-12-31')\n",
        "ARWR = ARWR.drop('Adj Close',axis=1); ARWR['symbol'] = 'ARWR'\n",
        "price_data = pd.concat([JPM,COST,IBM,HD,ARWR]); print('Número de LinhasxColunas:',price_data.shape);price_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSYAi1n-JBBv"
      },
      "source": [
        "### Data Preprocessing: Load the Data\n",
        "This portion is a little unnecessary because you take the function up above, and it will return the `price_data` data frame and use that. However, if you don't want to go through the process of pulling the data again. What we can do is load the CSV file we used previously. In this portion, I check if the `price_data.csv` file exists in the directory, and if it does, I load it. Otherwise, I call the `get_price_data` and grab the data again.\n",
        "\n",
        "Finally, I print the `price_data` data frame to verify the data was loaded. `YOU WILL NEED TO RUN THE FUNCTION EVERY DAY TO GET THE LATEST PRICE DATA.`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_0UpNZPJBBv"
      },
      "outputs": [],
      "source": [
        "### 5\n",
        "#if os.path.exists('price_data.csv'):\n",
        "\n",
        "    # Load the data\n",
        "    price_data = pd.read_csv('price_data.csv')\n",
        "\n",
        "#else:\n",
        "\n",
        "    # Grab the data and store it.\n",
        "    grab_price_data()\n",
        "\n",
        "    # Load the data\n",
        "    price_data = pd.read_csv('price_data.csv')\n",
        "\n",
        "# Display the head before moving on.\n",
        "#price_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_4o5NjJBBv"
      },
      "source": [
        "With the data now loaded, we can transform so we can calculate some of our trading indicators. The first thing we need to do is sort the data because we have multiple ticker symbols inside of our data frame. Take the data frame and call the `sort_values` method and specify the columns you wish to sort by using the `by` argument. In our case, we will be using a list of column names to sort by. The first sort is by the `symbol` column, and the second sort is by the `datetime` column.\n",
        "\n",
        "Once we've sorted the data, we need to calculate the change in price from one period to the next. To do this, we will use the `diff()` method. Grab the `close` column and call the `diff()` method. The `diff()` method will calculate the difference from one row to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSgtEHmIJBBw"
      },
      "outputs": [],
      "source": [
        "# I Just need the Close\n",
        "### 6\n",
        "price_data.reset_index(inplace=True)\n",
        "price_data = price_data.rename(columns = {'Date':'datetime','Close':'close','High':'high','Low':'low','Open':'open','Volume':'volume'})\n",
        "\n",
        "price_data = price_data[['symbol','datetime','close','high','low','open','volume']]\n",
        "\n",
        "\n",
        "'''\n",
        "    First, for average investors, the return of an asset is a complete and scale–free\n",
        "    summary of the investment opportunity. Second, return series are easier to\n",
        "    handle than prices series as they have more attractive statistical properties\n",
        "'''\n",
        "\n",
        "\n",
        "# sort the values by symbol and then date\n",
        "price_data.sort_values(by = ['symbol','datetime'], inplace = True)\n",
        "\n",
        "# calculate the change in price\n",
        "price_data['change_in_price'] = price_data['close'].diff()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO-lFp9sJBBw"
      },
      "source": [
        "### Data Preprocessing: Ticker Symbol\n",
        "Okay, so we've created our `change_in_price` column, but we need to do an extra step. Technically, each row where the ticker symbol changes are incorrect because it's using the price from a different ticker. That means we need to have the first row of each ticker symbol be `Nan` for the `change_in_price` column. To do this, we need to break out into steps.\n",
        "\n",
        "Step 1: Identify the rows where the ticker symbol changes. If we use the `shift()` method and shift every row down by one, the rows where unshifted column DOES NOT EQUAL the shifted column is where the ticker changed. We will store these values in a variable called `mask`.\n",
        "\n",
        "Step 2: Change those rows to `NaN` values. We can use the `numpy.where()` method to test our series. The test is simple, wherever the `mask` variable equals `True`, in other words, wherever the ticker symbol is different, set the `change_in_price` column to `np.nan`.\n",
        "\n",
        "After doing that, we can filter those `NaN` values; we should only have 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvocFILkJBBw"
      },
      "outputs": [],
      "source": [
        "# identify rows where the symbol changes\n",
        "mask = price_data['symbol'] != price_data['symbol'].shift(1)\n",
        "\n",
        "# For those rows, let's make the value null\n",
        "price_data['change_in_price'] = np.where(mask == True, np.nan, price_data['change_in_price'])\n",
        "\n",
        "# print the rows that have a null value, should only be 5\n",
        "price_data[price_data.isna().any(axis = 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFveemQbJBBw"
      },
      "source": [
        "### Data Preprocessing: Smoothing the Data (OPTIONAL)\n",
        "\n",
        "***\n",
        "**This part is optional, in the example below I will not be doing any smoothing of the data this is simply to give you the code necessary to reproduce certain results in the paper.**\n",
        "***\n",
        "\n",
        "In the paper, they test the model using different windows. For example, they make predictions 30 days out, 60 days out and 90 days out. To make this type of prediction, we have to transform the data so that when we pass it through the model it will be able to make those predictions that far out. The transformation they use is smoothing factor that is defined by the following:\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "\\begin{align}\n",
        "\\ S_{0} = Y_{0}\n",
        "\\\\\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\ \\text{for }t > 0\\text{, } S_{t} = \\alpha * Y_{t} + ( 1 - \\alpha ) * S_{t-1}\n",
        "\\\n",
        "\\end{align}\n",
        "\n",
        "where α is the smoothing factor and 0 < α < 1. Larger values of α reduce the level of smoothing. When α = 1, the smoothed statistic becomes equal to the actual observation. The goal of smoothing it remove the randomess and noise from our price data. In other words, we don't get a spiky up and down graph but instea a smoother one. Additionally,this will help the model to more easily identify long-term trends.\n",
        "\n",
        "Here is how to calculate the smoothed version of the prices using `pandas`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAu3Slh7JBBw"
      },
      "outputs": [],
      "source": [
        "# define the number of days out you want to predict\n",
        "days_out = 30\n",
        "\n",
        "# Group by symbol, then apply the rolling function and grab the Min and Max.\n",
        "price_data_smoothed = price_data.groupby(['symbol'])[['close','low','high','open','volume']].transform(lambda x: x.ewm(span = days_out).mean())\n",
        "\n",
        "# Join the smoothed columns with the symbol and datetime column from the old data frame.\n",
        "smoothed_df = pd.concat([price_data[['symbol','datetime']], price_data_smoothed], axis=1, sort=False)\n",
        "\n",
        "smoothed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NksgfIWfJBBw"
      },
      "source": [
        "### Data Preprocessing: Signal Flag\n",
        "\n",
        "If you chose to do the smoothing process then we need to add an additional column to our data frame. This will serve as `diff` column from the original data frame. However, in this case, we don't want one consecutive day to the next we want the number of days we want to predict out. What we will do is take the window we used up above to calculate our smoothed statistic and use it to calculate our signal flag.\n",
        "\n",
        "We will be using the `numpy.sign()` method which will return a `1.0` if positive, `-1.0` if negative and `0.0` if no change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnB-WaJEJBBw"
      },
      "outputs": [],
      "source": [
        "# define the number of days out you want to predict\n",
        "days_out = 30\n",
        "\n",
        "# create a new column that will house the flag, and for each group calculate the diff compared to 30 days ago. Then use Numpy to define the sign.\n",
        "smoothed_df['Signal_Flag'] = smoothed_df.groupby('symbol')['close'].transform(lambda x : np.sign(x.diff(days_out)))\n",
        "\n",
        "# print the first 50 rows\n",
        "smoothed_df.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Eo3RjPJBBx"
      },
      "source": [
        "### Indicator Calculation: Relative Strength Index (RSI)\n",
        "\n",
        "**Definition From Paper:**\n",
        "\n",
        "RSI is a popular momentum indicator that determines whether the stock is overbought or oversold. A stock is said to be overbought when the demand unjustifiably pushes\n",
        "the price upwards. This condition is generally interpreted as a sign that the stock is overvalued, and the price is likely to go down. A stock is said to be oversold when the price goes down sharply to a level below its true value. This is a result caused due to panic selling. RSI ranges from 0 to 100, and generally, when RSI is above 70, it may indicate that the stock is overbought and when RSI is below 30, it may indicate the stock is oversold.\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "\\begin{align}\n",
        "\\ RSI = 100 - \\frac{100}{1+RS}\n",
        "\\end{align}\n",
        "\n",
        "**Code:**\n",
        "\n",
        "From this point forward, a lot of the calculations will be mostly the same but only differ on the type of calculation we do. Each indicator is calculated using the same few steps:\n",
        "\n",
        "1. Copy the desired columns and store them in new variables.\n",
        "2. Group the columns by the `symbol`, select the column we wish to perform the transformation on and use the `transform` method along with a lambda function to calculate the indicator.\n",
        "3. Store the values in the main data frame.\n",
        "\n",
        "Now there might be a few extra steps in between, but the general idea is the same across each indicator. Now for the RSI indicator, I need to identify the up days and down days. Well, lucky for us, we already have the `change_in_price ` column, so we can use a condition that will set the value to 0 if the price went up for down days and vice versa for up days. After that, I need to make sure I have the absolute values for down days, or else the calculation won't be correct, so I modify that column and then calculate the EMA of both the Up and Down columns. Finally, I calculate the Relative strength metric and pass that through to the RSI calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN52fW4AJBBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the 14 day RSI\n",
        "n = 14\n",
        "\n",
        "# First make a copy of the data frame twice\n",
        "up_df, down_df = price_data[['symbol','change_in_price']].copy(), price_data[['symbol','change_in_price']].copy()\n",
        "\n",
        "# For up days, if the change is less than 0 set to 0.\n",
        "up_df.loc['change_in_price'] = up_df.loc[(up_df['change_in_price'] < 0), 'change_in_price'] = 0\n",
        "\n",
        "# For down days, if the change is greater than 0 set to 0.\n",
        "down_df.loc['change_in_price'] = down_df.loc[(down_df['change_in_price'] > 0), 'change_in_price'] = 0\n",
        "\n",
        "# We need change in price to be absolute.\n",
        "down_df['change_in_price'] = down_df['change_in_price'].abs()\n",
        "\n",
        "# Calculate the EWMA (Exponential Weighted Moving Average), meaning older values are given less weight compared to newer values.\n",
        "ewma_up = up_df.groupby('symbol')['change_in_price'].transform(lambda x: x.ewm(span = n).mean())\n",
        "ewma_down = down_df.groupby('symbol')['change_in_price'].transform(lambda x: x.ewm(span = n).mean())\n",
        "\n",
        "# Calculate the Relative Strength\n",
        "relative_strength = ewma_up / ewma_down\n",
        "\n",
        "# Calculate the Relative Strength Index\n",
        "relative_strength_index = 100.0 - (100.0 / (1.0 + relative_strength))\n",
        "\n",
        "# Add the info to the data frame.\n",
        "price_data['down_days'] = down_df['change_in_price']\n",
        "price_data['up_days'] = up_df['change_in_price']\n",
        "price_data['RSI'] = relative_strength_index\n",
        "\n",
        "# Display the head.\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YoKS8cJBBx"
      },
      "source": [
        "### Indicator Calculation: Stochastic Oscillator\n",
        "\n",
        "**Definition From Paper:**\n",
        "\n",
        "Stochastic Oscillator follows the speed or the momentum of the price. As a rule, momentum changes before the price changes. It measures the level of the closing price relative to the low-high range over a period of time.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\\begin{align}\n",
        "\\ K = 100 \\ast  \\frac{(C-L_{14})}{(H_{14}-L_{14})}\n",
        "\\end{align}\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ C = \\text{Current Closing Price}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ L_{14} = \\text{Lowest Low over the past 14 days}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ H_{14} = \\text{Highest High over the past 14 days}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "**Code:**\n",
        "\n",
        "The strategy here is pretty much the same; the only difference is the columns we are copying and the lambda function we are applying. For the RSI, we applied an EMA function, but for the Stochastic Oscillator, we use the rolling function. With this function, we specify our window, which in this case is 14 periods, and then specify measurement we want to apply to each window.\n",
        "\n",
        "After we obtained the max and min values, we pass it through our formula and apply the results to the main data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J00R2oVVJBBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the Stochastic Oscillator\n",
        "n = 14\n",
        "\n",
        "# Make a copy of the high and low column.\n",
        "low_14, high_14 = price_data[['symbol','low']].copy(), price_data[['symbol','high']].copy()\n",
        "\n",
        "# Group by symbol, then apply the rolling function and grab the Min and Max.\n",
        "low_14 = low_14.groupby('symbol')['low'].transform(lambda x: x.rolling(window = n).min())\n",
        "high_14 = high_14.groupby('symbol')['high'].transform(lambda x: x.rolling(window = n).max())\n",
        "\n",
        "# Calculate the Stochastic Oscillator.\n",
        "k_percent = 100 * ((price_data['close'] - low_14) / (high_14 - low_14))\n",
        "\n",
        "# Add the info to the data frame.\n",
        "price_data['low_14'] = low_14\n",
        "price_data['high_14'] = high_14\n",
        "price_data['k_percent'] = k_percent\n",
        "\n",
        "# Display the head.\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-nIopwVJBBx"
      },
      "source": [
        "### Indicator Calculation: Williams %R\n",
        "\n",
        "**Definition From Paper:**\n",
        "\n",
        "Williams %R ranges from -100 to 0. When its value is above -20, it indicates a sell signal and when its value is below -80, it indicates a buy signal.\n",
        "\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\\begin{align}\n",
        "\\ R = \\frac{(H_{14}-C)}{(H_{14}-L_{14})} * - 100\n",
        "\\end{align}\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ C = \\text{Current Closing Price}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ L_{14} = \\text{Lowest Low over the past 14 days}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ H_{14} = \\text{Highest High over the past 14 days}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "**Code:**\n",
        "\n",
        "Identical to the Stochastic Oscillator, we change the arrangement of the formula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9YHIIfDJBBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the Williams %R\n",
        "n = 14\n",
        "\n",
        "# Make a copy of the high and low column.\n",
        "low_14, high_14 = price_data[['symbol','low']].copy(), price_data[['symbol','high']].copy()\n",
        "\n",
        "# Group by symbol, then apply the rolling function and grab the Min and Max.\n",
        "low_14 = low_14.groupby('symbol')['low'].transform(lambda x: x.rolling(window = n).min())\n",
        "high_14 = high_14.groupby('symbol')['high'].transform(lambda x: x.rolling(window = n).max())\n",
        "\n",
        "# Calculate William %R indicator.\n",
        "r_percent = ((high_14 - price_data['close']) / (high_14 - low_14)) * - 100\n",
        "\n",
        "# Add the info to the data frame.\n",
        "price_data['r_percent'] = r_percent\n",
        "\n",
        "# Display the head.\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phQAQ19oJBBx"
      },
      "source": [
        "### Indicator Calculation: Moving Average Convergence Divergnece (MACD)\n",
        "\n",
        "**Definition From Paper:**\n",
        "\n",
        "EMA stands for Exponential Moving Average. When the MACD goes below the SingalLine, it indicates a sell signal. When it goes above the SignalLine, it indicates a buy signal.\n",
        "\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\ MACD = EMA_{12}(C) - EMA_{26}(C)\n",
        "\\end{align}\n",
        "\n",
        "\\begin{align}\n",
        "\\ SignalLine = EMA_{9}(MACD)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ MACD = \\text{Moving Average Convergence Divergence}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ C = \\text{Closing Price}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ EMA_{n} = \\text{n day Exponential Moving Average}\n",
        "\\end{align}\n",
        "\\\\\n",
        "$\n",
        "\n",
        "**Code:**\n",
        "\n",
        "For the MACD, we will need the `close` column, so grab that and then apply the `transform` method along with the specified Lambda function. Now calculating an Exponential Moving Average in `pandas` is easy. First, call the `ewm` (exponential moving weight) function and then specify the `span` or, in other words, the number of periods to look back. In this case, we use the definition provided by the formula and specify 26 & 12.\n",
        "\n",
        "Once we've calculated the EMA_26 and EMA_12, we take the difference between EMA_12 & EMA_26 to get our MACD. Now that we have our MACD, we need to calculate the EMA of the MACD, so we take our MACD series and apply the same `ewm` function too, but in this case, we specify a `span` of 9. Finally, we add both the MACD and MACD_EMA to the main data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN1FXFTsJBBx"
      },
      "outputs": [],
      "source": [
        "# Calculate the MACD\n",
        "ema_26 = price_data.groupby('symbol')['close'].transform(lambda x: x.ewm(span = 26).mean())\n",
        "ema_12 = price_data.groupby('symbol')['close'].transform(lambda x: x.ewm(span = 12).mean())\n",
        "macd = ema_12 - ema_26\n",
        "\n",
        "# Calculate the EMA\n",
        "ema_9_macd = macd.ewm(span = 9).mean()\n",
        "\n",
        "# Store the data in the data frame.\n",
        "price_data['MACD'] = macd\n",
        "price_data['MACD_EMA'] = ema_9_macd\n",
        "\n",
        "# Print the head.\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzdXSclHJBBy"
      },
      "source": [
        "### Indicator Calculation: Price Rate Of Change\n",
        "\n",
        "**Definition From Paper:**\n",
        "\n",
        "It measures the most recent change in price with respect to the price in `n` days ago.\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\\\\n",
        "\\ PROC_{t} = \\frac{C_{t} - C_{t-n}}{C_{t-n}}\n",
        "\\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ PROC_{t} = \\text{Price Rate of Change at time t}\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ C_{t} = \\text{Closing price at time t}\n",
        "\\end{align}\n",
        "\\\\\n",
        "$\n",
        "\n",
        "\n",
        "**Code:**\n",
        "\n",
        "The Price Rate of Change is another easy indicator to calculate in pandas because we can leverage a built-in function. In this case, we will use the `pct_change` function and apply it to our all too familiar symbol groups. For the `pct_change` function, we have an argument called `periods` which specifies how far we need to look back when calculating the rate of change. In this case, the paper never provided a specific `n,` but after doing some research, I landed on an `n` of 9 because this seemed to be the standard window. Now, it's important to note that the paper changes `n` depending on the window, so technically I'm not doing exactly like they did. For example, if my prediction window was `30 days` then `n` should be 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PMZoymtJBBy"
      },
      "outputs": [],
      "source": [
        "# Calculate the Price Rate of Change\n",
        "n = 9\n",
        "\n",
        "# Calculate the Rate of Change in the Price, and store it in the Data Frame.\n",
        "price_data['Price_Rate_Of_Change'] = price_data.groupby('symbol')['close'].transform(lambda x: x.pct_change(periods = n))\n",
        "\n",
        "# Print the first 30 rows\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5exQwgJBBy"
      },
      "source": [
        "### Indicator Calculation: On Balance Volume\n",
        "\n",
        "#### Definition From Paper:\n",
        "\n",
        "On balance volume (OBV) (Granville 1976) utilizes changes in volume to estimate changes in stock prices. This technical indicator is used to d buying and selling trends of a stock, by considering the cumulative volume: it cumulatively adds the volumes on days when the prices group, and subtracts the volume on the days when prices go down, compared to the prices of the previous day.\n",
        "\n",
        "#### Formula:\n",
        "\n",
        "$\n",
        "\\begin{equation}\n",
        "OBV (t) =\n",
        "    \\begin{cases}\n",
        "      \\text{OBV(t - 1) + Vol(t) if C(t) > C(t-1)}\\\\\n",
        "      \\text{OBV(t - 1) - Vol(t) if C(t) < C(t-1)}\\\\\n",
        "      \\text{OBV(t - 1) if C(t) = C(t-1)}\\\\\n",
        "    \\end{cases}\\\\\n",
        "\\\\\n",
        "\\text{where,}\\\\\n",
        "\\\\\n",
        "\\text{OBV (t) = on balance volume at time t}\\\\\n",
        "\\\\\n",
        "\\text{Vol(t) = trading volume at time t}\\\\\n",
        "\\\\\n",
        "\\text{C(t) = closing price at time t}\\\\\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "**Code:**\n",
        "\n",
        "This portion is a little more complicated than the previous ones. However, the idea is still the same. I'm going to be working with groups but in this case I'll be using the `apply` method to apply a custom function I built to calculate the `On Balance Volume`. The function simply calculates the `diff` for the closing price and uses a `for loop` to loop through each row in the `volume` column. If the `change in price`  was greater than 0 we add the volume, if it's less than 0 we subtract the volume and if it's 0 then we leave it alone.\n",
        "\n",
        "When I return the values I need to make sure it's a `pandas.Series` object with an `index`. Once I have the `pandas.Series` I just add it to the dataframe like in previous examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JkrvCRxJBBy"
      },
      "outputs": [],
      "source": [
        "def obv(group):\n",
        "\n",
        "    # Grab the volume and close column.\n",
        "    volume = group['volume']\n",
        "    change = group['close'].diff()\n",
        "\n",
        "    # intialize the previous OBV\n",
        "    prev_obv = 0\n",
        "    obv_values = []\n",
        "\n",
        "    # calculate the On Balance Volume\n",
        "    for i, j in zip(change, volume):\n",
        "\n",
        "        if i > 0:\n",
        "            current_obv = prev_obv + j\n",
        "        elif i < 0:\n",
        "            current_obv = prev_obv - j\n",
        "        else:\n",
        "            current_obv = prev_obv\n",
        "\n",
        "        # OBV.append(current_OBV)\n",
        "        prev_obv = current_obv\n",
        "        obv_values.append(current_obv)\n",
        "\n",
        "    # Return a panda series.\n",
        "    return pd.Series(obv_values, index = group.index)\n",
        "\n",
        "\n",
        "# apply the function to each group\n",
        "obv_groups = price_data.groupby('symbol').apply(obv)\n",
        "\n",
        "# add to the data frame, but drop the old index, before adding it.\n",
        "price_data['On Balance Volume'] = obv_groups.reset_index(level=0, drop=True)\n",
        "\n",
        "# display the data frame.\n",
        "price_data.head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naL4OaJFJBBy"
      },
      "source": [
        "### Building the Model: Creating the Prediction Column\n",
        "Now that we have our technical indicators calculated and our price data cleaned up, we are almost ready to build our model. However, we are missing one critical piece of information that is crucial to the model, the column we wish to predict. Now at this point, our data frame doesn't have that column, but we will create it before we feed the data into the model.\n",
        "\n",
        "However, before we create it, I want to take some time and understand the exact problem we are trying to solve. Our goal is to predict whether the next day is either a `down_day` or an `up_day`. Based on this knowledge, we are solving a classification problem. If you don't remember, there are two categories of problems in machine learning, classification, and regression. With classification problems, we try to predict which group new values belong to. For example, based on store sales, are they `outperforming store` or an `underperforming store`. Classification problems are problems that have discrete groups. With regression problems, we are trying to predict non-discrete values and, for example, trying to forecast future sales based on previous sales.\n",
        "\n",
        "In our case, we have a classification problem because we have two discrete groups, `up_days` and `down_days`, and our goal is to take new values (new prices) and classify them into these two groups based on their values.\n",
        "\n",
        "To create our prediction column, we will group our data frame by each `symbol`. After we've created our groups, we need to select the `close` column as this contains the price we need to determine if the stock closed up or down for any given day. Now, we can use a similar logic we used to calculate the price change. However, in this case, we only need to know if the price is higher or lower compared to the previous day.\n",
        "\n",
        "Take your groups, use the `transform` method to apply a lambda function to your groups. The lambda function will use the `diff()` function to compare the current price to the previouse price. We then wrap the results of that function in the `numpy.sign()` function. That function will return `1.0` for negative values (down days), `1.0` for postive values, and `0.0` for no change (flat days)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIKOBsRiJBBy"
      },
      "outputs": [],
      "source": [
        "# Create a column we wish to predict\n",
        "'''\n",
        "    In this case, let's create an output column that will be 1 if the closing price at time 't' is greater than 't-1' and 0 otherwise.\n",
        "    In other words, if the today's closing price is greater than yesterday's closing price it would be 1.\n",
        "'''\n",
        "\n",
        "# Group by the `Symbol` column, then grab the `Close` column.\n",
        "close_groups = price_data.groupby('symbol')['close']\n",
        "\n",
        "# Apply the lambda function which will return -1.0 for down, 1.0 for up and 0.0 for no change.\n",
        "close_groups = close_groups.transform(lambda x : np.sign(x.diff()))\n",
        "\n",
        "# add the data to the main dataframe.\n",
        "price_data['Prediction'] = close_groups\n",
        "\n",
        "# for simplicity in later sections I'm going to make a change to our prediction column. To keep this as a binary classifier I'll change flat days and consider them up days.\n",
        "price_data.loc[price_data['Prediction'] == 0.0] = 1.0\n",
        "\n",
        "# print the head\n",
        "price_data.head(50)\n",
        "\n",
        "# OPTIONAL CODE: Dump the data frame to a CSV file to examine the data yourself.\n",
        "# price_data.to_csv('final_metrics.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3PzgDFQJBBy"
      },
      "source": [
        "### Building the Model: Removing `NaN` Values\n",
        "\n",
        "The random forest can't accept `Nan` values, so we will need to remove them before feeding the data in. The code below prints the number of rows before dropping the `NaN` values, use the `dropna` method to remove any rows `NaN` values and then displays the number of rows after dropping the `NaN` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N6RFsSPJBBy"
      },
      "outputs": [],
      "source": [
        "# We need to remove all rows that have an NaN value.\n",
        "print('Before NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n",
        "\n",
        "# Any row that has a `NaN` value will be dropped.\n",
        "price_data = price_data.dropna()\n",
        "\n",
        "# Display how much we have left now.\n",
        "print('After NaN Drop we have {} rows and {} columns'.format(price_data.shape[0], price_data.shape[1]))\n",
        "\n",
        "# Print the head.\n",
        "price_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Yi-AgBJBBy"
      },
      "source": [
        "### Building the Model: Splitting the Data\n",
        "If you remember back to our series on regression analysis, we have split our data into a training set and testing set. For Random Forest, we need to do the same, so we need to identify our input columns which are the following:\n",
        "\n",
        "1. RSI\n",
        "2. Stochastic Oscillator\n",
        "3. William %R\n",
        "4. Price Rate of Change\n",
        "5. MACD\n",
        "\n",
        "Those columns will serve as our `X`, and our `Y` column will be the `Prediction` column, the column that specifies whether the stock closed up or down compared to the previous day.\n",
        "\n",
        "Once we've selected our columns, we need to split the data into a training and test set. SciKit learn makes this easy by providing the `train_test_split` object, which will take our `X_Cols` and `Y_Cols` and split them based on the size we input. In our case, let's have the `test_size` be '20 %`. For reproducibility, the `train_test_split` object provides the `random_state` argument that will split the data along the same dimensions every time.\n",
        "\n",
        "After we've split the data, we can create our `RandomForestClassifier` model.\n",
        "\n",
        "Once we've created it, we can `fit` the training data to the model using the `fit` method. Finally, with our \"trained\" model, we can make predictions. Take the `X_test` data set and use it to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU5ej3raJBBy"
      },
      "outputs": [],
      "source": [
        "# Grab our X & Y Columns.\n",
        "X_Cols = price_data[['RSI','k_percent','r_percent','Price_Rate_Of_Change','MACD','On Balance Volume']]\n",
        "Y_Cols = price_data['Prediction']\n",
        "\n",
        "# Split X and y into X_\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, random_state = 0)\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rand_frst_clf = RandomForestClassifier(n_estimators = 100, oob_score = True, criterion = \"gini\", random_state = 0)\n",
        "\n",
        "# Fit the data to the model\n",
        "rand_frst_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rand_frst_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUeo37a3JBBz"
      },
      "source": [
        "### Model Evaluation: Accuracy\n",
        "We've built our model, so let's see how accurate it is. SciKit learn, again, makes the process of evaluating our model very easy by providing a bunch of built-in metrics that we can call.\n",
        "\n",
        "One of those metrics is the `accuracy_score`.\n",
        "\n",
        "The accuracy_score function computes the accuracy, either the fraction (default) or the count (`normalize`=`False`) of correct predictions. Accuracy is defined as the number of accurate predictions the model made on the test set. Imagine we had three **`TRUE`** values `[1, 2, 3]`, and our model predicted the following values `[1, 2, 4]` we would say the accuracy of our model is `66 %`.\n",
        "\n",
        "Now I can almost guarantee that your accuracy will be different than mine because if you don't run this model using the same data set as I did, then it could be higher or lower. In my case, the model accuracy was around `71.1%`, which is pretty high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHl6HzXZJBBz"
      },
      "outputs": [],
      "source": [
        "# Print the Accuracy of our Model.\n",
        "print('Correct Prediction (%): ', accuracy_score(y_test, rand_frst_clf.predict(X_test), normalize = True) * 100.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIO6mDW5JBBz"
      },
      "source": [
        "### Model Evaluation: Classification Report\n",
        "To get a more detailed overview of how the model performed, we can build a classification report that will compute the `F1_Score`, the `Precision`, the `Recall`, and the `Support`. Now, I'm assuming you don't know what these metrics are, so let's take some time to go over them.\n",
        "\n",
        "### Accuracy:\n",
        "Accuracy measures the portion of all testing samples classified correctly and is defined as the following:\n",
        "\n",
        "\\begin{align}\n",
        "\\\\\n",
        "\\ Accuracy = \\frac{tp + tn}{(tp +tn)+(fp-fn)}\n",
        "\\\\\n",
        "\\end{align}\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\\\\n",
        "\\ tp = \\text{True Positive}\n",
        "\\\\\n",
        "\\ tn = \\text{True Negative}\n",
        "\\\\\n",
        "\\ fp = \\text{False Positive}\n",
        "\\\\\n",
        "\\ fn = \\text{False Negative}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "### Recall\n",
        "Recall (also known as sensitivity) measures the ability of a classifier to correctly identify positive labels and is defined as the following:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\\\\n",
        "\\ Recall = \\frac{tp}{(tp +fn)}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\\\\n",
        "\\ tp = \\text{True Positive}\n",
        "\\\\\n",
        "\\ fn = \\text{False Negative}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1, and the worst value is 0.\n",
        "\n",
        "### Specificity\n",
        "Specificity measures the classifier’s ability to correctly identify negative labels and is defined as the following:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\\\\n",
        "\\ Specificity = \\frac{tn}{(tn +fp)}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\\\\n",
        "\\ tn = \\text{True Negative}\n",
        "\\\\\n",
        "\\ fp = \\text{False Positive}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "### Precision\n",
        "Precision measures the proportion of all correctly identified samples in a population of samples which are classified as positive labels and is defined as the following:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\\\\n",
        "\\ Percision = \\frac{tp}{(tp +fp)}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\ where,\n",
        "\\\\\n",
        "\\ tp = \\text{True Positive}\n",
        "\\\\\n",
        "\\ fp = \\text{False Positive}\n",
        "\\\\\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1, and the worst value is 0.\n",
        "\n",
        "\n",
        "### Interpreting the Classification Report\n",
        "Now the fun part, interpretation. When it comes to evaluating the model, there we generally look at the accuracy. If our accuracy is high, it means our model is correctly classifying items.\n",
        "\n",
        "In some cases, we will have models that may have low precision or high recall. It's difficult to compare two models with low precision and high recall or vice versa. To make results comparable, we use a metric called the F-Score. The F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfElEFJHJBBz"
      },
      "outputs": [],
      "source": [
        "# Define the target names\n",
        "target_names = ['Down Day', 'Up Day']\n",
        "\n",
        "# Build a classifcation report\n",
        "report = classification_report(y_true = y_test, y_pred = y_pred, target_names = target_names, output_dict = True)\n",
        "\n",
        "# Add it to a data frame, transpose it for readability.\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwlU9nC0JBBz"
      },
      "source": [
        "### Model Evaluation: Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foKhBvV8JBB9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "rf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "true_negatives = rf_matrix[0][0]\n",
        "false_negatives = rf_matrix[1][0]\n",
        "true_positives = rf_matrix[1][1]\n",
        "false_positives = rf_matrix[0][1]\n",
        "\n",
        "accuracy = (true_negatives + true_positives) / (true_negatives + true_positives + false_negatives + false_positives)\n",
        "percision = true_positives / (true_positives + false_positives)\n",
        "recall = true_positives / (true_positives + false_negatives)\n",
        "specificity = true_negatives / (true_negatives + false_positives)\n",
        "\n",
        "print('Accuracy: {}'.format(float(accuracy)))\n",
        "print('Percision: {}'.format(float(percision)))\n",
        "print('Recall: {}'.format(float(recall)))\n",
        "print('Specificity: {}'.format(float(specificity)))\n",
        "\n",
        "#disp = plot_confusion_matrix(rand_frst_clf, X_test, y_test, display_labels = ['Down Day', 'Up Day'], normalize = 'true', cmap=plt.cm.Blues)\n",
        "#disp.ax_.set_title('Confusion Matrix - Normalized')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=rf_matrix, display_labels=rand_frst_clf.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6ecWLqoJBB9"
      },
      "source": [
        "### Model Evaluation: Feature Importance\n",
        "With any model, you want to have an idea of what features are helping explain most of the model, as this can give you insight as to why you're getting the results you are. With Random Forest, we can identify some of our most important features or, in other words, the features that help explain most of the model. In some cases, some of our features might not be very important, or in other words, when compared to additional features, don't explain much of the model.\n",
        "\n",
        "### Why Do We Care About Feature Importance?\n",
        "What that means is if we were to get rid of those features, our accuracy will go down a little, hopefully, but not significantly. You might be asking, \"Why would I want to get rid of a feature if it lowers my accuracy?\" Well, it depends, in some cases, you don't care if your model is 95% accurate or 92% accurate. To you, a 92% accurate model is just as good as a 95% accurate model.\n",
        "\n",
        "However, if you wanted to get a 95% accurate model, you would, in this hypothetical case, have to train your model twice as long. Now, I'm a little extreme in this case, but the idea is the same. The cost doesn't justify the benefit. In the real world, we have to make these decisions all the time, and in some cases, it just doesn't warrant the extra cost for such a minimal increase in the accuracy.\n",
        "\n",
        "### Calculating the Feature Importance\n",
        "Like all the previous steps, SkLearn makes this process very easy. Take your `rand_frst_clf` and call the `feature_importances_` property. This will return all of our features and their importance measurement. Store the values in a `Pandas.Series` object and sore the values.\n",
        "\n",
        "Feature importance can be calculated two ways in Random Forest:\n",
        "\n",
        "1. Gini-Based Importance\n",
        "2. Accuracy-Based Importance\n",
        "\n",
        "Here is how both measures of importance are calculated.\n",
        "\n",
        "With `sklearn` they use the Gini-Importance metric for the Random Forest Algorithm.\n",
        "\n",
        "We can see in our model, that the most important feature is `k_percent` and our least important feature is `Price_Rate_Of_Change`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isUh2KkeJBB-"
      },
      "outputs": [],
      "source": [
        "# Calculate feature importance and store in pandas series\n",
        "feature_imp = pd.Series(rand_frst_clf.feature_importances_, index=X_Cols.columns).sort_values(ascending=False)\n",
        "feature_imp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NISPlV5wJBB-"
      },
      "source": [
        "### Model Evaluation: Feature Importance Graphing\n",
        "If you want, you can also graph the feature importance of our model, so it's a little easier to visualize the results. What I do in the chart below is chart the cumulative importance or, in other words, how much does each feature add to the total. That way, we can see how much each feature is contributing to the overall importance. Another standard graph that is used is a bar chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkkjJLT3JBB-"
      },
      "outputs": [],
      "source": [
        "# store the values in a list to plot.\n",
        "x_values = list(range(len(rand_frst_clf.feature_importances_)))\n",
        "\n",
        "# Cumulative importances\n",
        "cumulative_importances = np.cumsum(feature_imp.values)\n",
        "\n",
        "# Make a line graph\n",
        "plt.plot(x_values, cumulative_importances, 'g-')\n",
        "\n",
        "# Draw line at 95% of importance retained\n",
        "plt.hlines(y = 0.95, xmin = 0, xmax = len(feature_imp), color = 'r', linestyles = 'dashed')\n",
        "\n",
        "# Format x ticks and labels\n",
        "plt.xticks(x_values, feature_imp.index, rotation = 'vertical')\n",
        "\n",
        "# Axis labels and title\n",
        "plt.xlabel('Variable')\n",
        "plt.ylabel('Cumulative Importance')\n",
        "plt.title('Random Forest: Feature Importance Graph')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYpoIbj5JBB-"
      },
      "source": [
        "### Model Evaluation: ROC Curve\n",
        "The Receiver Operating Characteristic is a graphical method to evaluate the performance of a binary classifier. A curve is drawn by plotting True Positive Rate (sensitivity) against False Positive Rate (1 - specificity) at various threshold values. ROC curve shows the trade-off between sensitivity and specificity. When the curve comes closer to the left-hand border and the top border of the ROC space, it indicates that the test is accurate. The closer the curve is to the top and left-hand border, the more accurate the test is. If the curve is close to the 45 degrees diagonal of the ROC space, it means that the test is not accurate. ROC curves can be used to select the optimal model and discard the suboptimal ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8uzXy-uJBB-"
      },
      "outputs": [],
      "source": [
        "# Create an ROC Curve plot.\n",
        "\n",
        "#rfc_disp = plot_roc_curve(rand_frst_clf, X_test, y_test, alpha = 0.8)\n",
        "rfc_disp = RocCurveDisplay.from_estimator(rand_frst_clf, X_test, y_test, alpha = 0.8)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlnnO6xGJBB-"
      },
      "source": [
        "### Model Evaluation: Out-Of-Bag Error Score\n",
        "The oob_score uses a sample of \"left-over\" data that wasn't necessarily used during the model's analysis, and the validation set is a sample of data you decided to subset. In this way, the OOB sample is a little more random than the validation set. Therefore, the OOB sample (on which the oob_score is measured) may be \"harder\" that the validation set. The oob_score may, on average, have a \"less good\" accuracy score as a consequence.\n",
        "\n",
        "For example, Jeremy and Terence use only the last 2 weeks of grocery store data as a validation set. The OOB sample may have unused data from across all four years of sales data. The oob_score's sample is much harder because it's more randomized and has more variance.\n",
        "\n",
        "If the oob_score never improves, but the validation set score is always excellent. You need to re-think how to subset the validation set. In the case of Jeremy and Terence, they might decide to take a more random sample of data across all years rather than strictly the last 2 weeks of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k340fn8JBB-"
      },
      "outputs": [],
      "source": [
        "print('Random Forest Out-Of-Bag Error Score: {}'.format(rand_frst_clf.oob_score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv4UHmxAJBB-"
      },
      "source": [
        "### Model Improvement: Randomized Search\n",
        "If you remember way up above, I mentioned that there is no magic number of estimators to use with every data set. Instead, with Random Forest, we have to try different values to find what the optimal values should be for each of the parameters. Fortunately, for us, this can be done using the `RandomizedSearchCV` method provided by `sklearn`.\n",
        "\n",
        "The idea behind this approach is to provide a wide range of possible values for each hyperparameter and then using cross-validation, to try different combinations of these parameters. With the highest result of these combinations being the one, we should use for our data set.\n",
        "\n",
        "To use this method, we need to first import the `RandomizedSearchCV` object from the `sklearn.model_selection` module. From there, we need to define a range of values for each of the hyperparameters we wish to test. To do this appropriately, we need to make sure we understand what each argument means in the model, so let's walk through them:\n",
        "\n",
        "1. n_estimators - The number of trees in the forest.\n",
        "2. max_features - The number of features to consider when looking for the best split.\n",
        "3. max_depth - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "4. min_samples_split - The minimum number of samples required to split an internal node.\n",
        "5. min_samples_leaf - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "6. bootstrap - Whether bootstrap samples are used when building trees. If False, the whole dataset is used to construct each tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3xTfL69JBB-"
      },
      "outputs": [],
      "source": [
        "# Number of trees in random forest\n",
        "# Number of trees is not a parameter that should be tuned, but just set large enough usually. There is no risk of overfitting in random forest with growing number of # trees, as they are trained independently from each other.\n",
        "n_estimators = list(range(200, 2000, 200))\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt', None, 'log2']\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "# Max depth is a parameter that most of the times should be set as high as possible, but possibly better performance can be achieved by setting it lower.\n",
        "max_depth = list(range(10, 110, 10))\n",
        "max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "# Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. Too high values can also lead to # under-fitting hence depending on the level of underfitting or overfitting, you can tune the values for min_samples_split.\n",
        "min_samples_split = [2, 5, 10, 20, 30, 40]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 7, 12, 14, 16 ,20]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "print(random_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUvkieDJJBB-"
      },
      "source": [
        "### Model Improvement: Running Randomized Search\n",
        "Now that we've created a range of values for some of our hyperparameters, we can put them to the test. The first thing we need to do is create a new instance of our `RandomForestClassifier` model and pass it through to our `RandomizedSearchCV` object. When we use the `RandomizedSearchCV`, we need to specify a few additional arguments.\n",
        "\n",
        "The estimator is the model we wish to use; in this case, it's just our `RandomForestClassifier`. The `param_distribution` will get our `random_grid` dictionary. `n_iter` is an important argument because it will specify the number of iterations we will do, so the higher it is, the more iterations we will do. `cv`, defines the cross-validation splitting strategy we will use, `random_state` is used for random uniform sampling. `verbose` controls the verbosity: the higher, the more messages. `n_jobs` number of jobs to run in parallel. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9rhxZ3rWJBB-"
      },
      "outputs": [],
      "source": [
        "# New Random Forest Classifier to house optimal parameters\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Specfiy the details of our Randomized Search\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaXPRGvvJBB-"
      },
      "outputs": [],
      "source": [
        "# With the new Random Classifier trained we can proceed to our regular steps, prediction.\n",
        "rf_random.predict(X_test)\n",
        "\n",
        "\n",
        "'''\n",
        "    ACCURACY\n",
        "'''\n",
        "# Once the predictions have been made, then grab the accuracy score.\n",
        "print('Correct Prediction (%): ', accuracy_score(y_test, rf_random.predict(X_test), normalize = True) * 100.0)\n",
        "\n",
        "\n",
        "'''\n",
        "    CLASSIFICATION REPORT\n",
        "'''\n",
        "# Define the traget names\n",
        "target_names = ['Down Day', 'Up Day']\n",
        "\n",
        "# Build a classifcation report\n",
        "report = classification_report(y_true = y_test, y_pred = y_pred, target_names = target_names, output_dict = True)\n",
        "\n",
        "# Add it to a data frame, transpose it for readability.\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "display(report_df)\n",
        "print('\\n')\n",
        "\n",
        "'''\n",
        "    FEATURE IMPORTANCE\n",
        "'''\n",
        "# Calculate feature importance and store in pandas series\n",
        "feature_imp = pd.Series(rand_frst_clf.feature_importances_, index=X_Cols.columns).sort_values(ascending=False)\n",
        "display(feature_imp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXKfuB1wJBB_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "    ROC CURVE\n",
        "'''\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Create an ROC Curve plot.\n",
        "rfc_disp = plot_roc_curve(rand_frst_clf, X_test, y_test, alpha = 0.8, name='ROC Curve', lw=1, ax=ax)\n",
        "\n",
        "# Add our Chance Line\n",
        "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
        "\n",
        "# Make it look pretty.\n",
        "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"ROC Curve Random Forest\")\n",
        "\n",
        "# Add the legend to the plot\n",
        "ax.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo Vídeo + Colab em PT"
      ],
      "metadata": {
        "id": "j1qJlSjD4Ydm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://youtu.be/RtA1rjhuavs"
      ],
      "metadata": {
        "id": "Dq3Jvwdq4XzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo DataCamp"
      ],
      "metadata": {
        "id": "NMZkm9Dq7l5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disponível em https://www.datacamp.com/tutorial/random-forests-classifier-python"
      ],
      "metadata": {
        "id": "AkwmaL__7qmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Curva de Aprendizagem vs Curva de Perda"
      ],
      "metadata": {
        "id": "_-amstEY8iWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Disponível em https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
        "\n",
        "#Desafio: encontrar informações sobre curva de perda para modelo de Random Forest."
      ],
      "metadata": {
        "id": "ME_xbFsP8maW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Código deste curso"
      ],
      "metadata": {
        "id": "FD4xdaY590ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelo para Classificação"
      ],
      "metadata": {
        "id": "yXnOOazIiMKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar bibliotecas básicas\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F8zuIs4S98y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parâmetros para a coleta\n",
        "import yfinance as yf #biblioteca p/ coleta de dados\n",
        "\n",
        "dataini = '2000-01-01'\n",
        "datafim = '2022-12-31'\n",
        "ticker = 'PETR4.SA'\n",
        "df = yf.download(ticker, start=dataini, end=datafim)\n",
        "df['dif'] = df['Close'].diff() # calcula diferença no fechamento de um dia pro outro\n",
        "#definição da variável dependente. No caso, h é o horizonte de previsão.\n",
        "h=3\n",
        "df['Y']  = np.where((df['Close'].shift(-h)- df['Close'])>0, 1, 0)\n",
        "df = df.dropna()\n",
        "#df.head(10)"
      ],
      "metadata": {
        "id": "ZiWIcsEj-dWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab our X & Y Columns.\n",
        "X_Cols = df.drop('Y',axis=1)\n",
        "Y_Cols = df['Y']\n",
        "\n",
        "# Split X and y\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, random_state = 0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, test_size= 0.2 ,shuffle=False)\n",
        "#print(X_train.head());print(X_train.tail());print(X_test.head()); print(X_test.tail())"
      ],
      "metadata": {
        "id": "iycZYQ2sd4Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Random Forest Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators = 100, oob_score = True, criterion = \"gini\", random_state = 0)\n",
        "\n",
        "# Fit the data to the model\n",
        "rf_class = rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_class.predict(X_test)\n",
        "\n",
        "#Qualidade do Modelo\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Troca 0/1 no Y por queda/alta\n",
        "target_names = ['Queda', 'Alta']\n",
        "print(classification_report(y_true = y_test, y_pred = y_pred, target_names = target_names))"
      ],
      "metadata": {
        "id": "i1KBnKH4f1s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Obter a importância das características\n",
        "feature_importance = rf_class.feature_importances_\n",
        "\n",
        "# Obter os nomes das características\n",
        "feature_names = X_Cols.columns\n",
        "\n",
        "# Criar um DataFrame para facilitar a visualização\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Criar um gráfico de barras para visualizar a importância das características\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance do Modelo Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "axKfCTF-IuF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(rf_class, X_Cols, Y_Cols,\n",
        "            shuffle=False, scoring=\"accuracy\", train_sizes = [1, 100, 500, 1000, 2000, 3000, 4000]\n",
        ")\n",
        "\n",
        "plt.plot(train_sizes,np.mean(train_scores,axis=1), label=\"train\")\n",
        "plt.plot(train_sizes,np.mean(test_scores,axis=1), label=\"test\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JZ9uFqROJFuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo e acompanhar a curva de perda\n",
        "# Atenção, esse processo costuma demorar alguns minutos.\n",
        "# Obs: extraído do ChatGPT com adaptações.\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for i in range(1, n_estimators + 1):\n",
        "    rf_model.set_params(n_estimators=i)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    train_loss = np.mean((y_train - rf_model.predict(X_train)) ** 2)\n",
        "    test_loss = np.mean((y_test - rf_model.predict(X_test)) ** 2)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Plotar a curva de perda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_estimators + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, n_estimators + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Random Forest Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RC3bdPxxJXhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelo de Regressão"
      ],
      "metadata": {
        "id": "CBnFHBUmiZgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar bibliotecas básicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "a8N-A8JcidWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parâmetros para a coleta\n",
        "import yfinance as yf #biblioteca p/ coleta de dados\n",
        "\n",
        "dataini = '2000-01-01'\n",
        "datafim = '2022-12-31'\n",
        "ticker = 'PETR4.SA'\n",
        "df = yf.download(ticker, start=dataini, end=datafim)\n",
        "df['dif'] = df['Close'].diff() # calcula diferença no fechamento de um dia pro outro\n",
        "#definição da variável dependente. No caso, h é o horizonte de previsão.\n",
        "h=3\n",
        "df['Y']  = df['Close'].shift(-h)\n",
        "df= df.dropna()\n",
        "#df.head(10)"
      ],
      "metadata": {
        "id": "me4uKrPVjdjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab our X & Y Columns.\n",
        "X_Cols = df.drop('Y',axis=1)\n",
        "Y_Cols = df['Y']\n",
        "\n",
        "# Split X and y\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, random_state = 0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, test_size= 0.2 ,shuffle=False)\n",
        "#print(X_train.head());print(X_train.tail());print(X_test.head()); print(X_test.tail())"
      ],
      "metadata": {
        "id": "6GQaI-Sbj4_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "n_estimators = 100\n",
        "rf_model = RandomForestRegressor(n_estimators = n_estimators, random_state = 0)\n",
        "rf_reg = rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "#Qualidade do Modelo\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
        "\n",
        "print('MAPE =', round(mean_absolute_percentage_error(y_test, y_pred)*100,2), '%')\n",
        "print('MAE =', round(r2_score(y_test, y_pred),2))\n",
        "print('RMSE =', round(mean_squared_error(y_test, y_pred)**.5,2))"
      ],
      "metadata": {
        "id": "8zEgI-yokvYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter a importância das características\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Obter os nomes das características\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Criar um DataFrame para facilitar a visualização\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Criar um gráfico de barras para visualizar a importância das características\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance do Modelo Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rpatRH5MF37E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(rf_reg, X_Cols, Y_Cols,\n",
        "            shuffle=False, scoring=\"neg_root_mean_squared_error\", train_sizes = [1, 100, 500, 1000, 1500, 2000, 2300]\n",
        ")\n",
        "\n",
        "plt.plot(train_sizes,np.mean(train_scores,axis=1), label=\"train\")\n",
        "plt.plot(train_sizes,np.mean(test_scores,axis=1), label=\"test\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_bDKm_sHmygG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo e acompanhar a curva de perda\n",
        "# Atenção, esse processo costuma demorar alguns minutos.\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for i in range(1, n_estimators + 1):\n",
        "    rf_model.set_params(n_estimators=i)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    train_loss = np.mean((y_train - rf_model.predict(X_train)) ** 2)\n",
        "    test_loss = np.mean((y_test - rf_model.predict(X_test)) ** 2)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Plotar a curva de perda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_estimators + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, n_estimators + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Random Forest Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xHyamN2X4pcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Proposta"
      ],
      "metadata": {
        "id": "jfVGpPIXN7J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Incluindo outras variáveis\n",
        "!pip install -q -U pandas_ta"
      ],
      "metadata": {
        "id": "1WeIWwWb4GCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas_ta as ta\n",
        "\n",
        "data=df\n",
        "data['RSI']=ta.rsi(data['Close'], length = 14)\n",
        "data['stoch'] = ta.stoch(data['High'], data['Low'], data['Close']).iloc[:,0]\n",
        "data['willr'] = ta.willr(data['High'], data['Low'], data['Close'])\n",
        "data['macd'] = ta.macd(data['Close']).iloc[:,0]\n",
        "data['macd_ema'] = ta.ema(data['macd'], length=9)\n",
        "data['roc'] = ta.roc(data['Close'], length=9)\n",
        "data['obv'] = ta.obv(data['Close'], data['Volume'])\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "d_h1pTDH4VUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab our X & Y Columns.\n",
        "X_Cols = data.drop(['Y', 'Adj Close'],axis=1)\n",
        "Y_Cols = data['Y']\n",
        "\n",
        "# Split X and y\n",
        "from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, random_state = 0)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_Cols, Y_Cols, test_size= 0.2 ,shuffle=False)\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "n_estimators = 100\n",
        "rf_model = RandomForestRegressor(n_estimators = n_estimators, random_state = 0)\n",
        "rf_reg = rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_reg.predict(X_test)\n",
        "\n",
        "#Qualidade do Modelo\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
        "\n",
        "print('MAPE =', round(mean_absolute_percentage_error(y_test, y_pred)*100,2), '%')\n",
        "print('MAE =', round(r2_score(y_test, y_pred),2))\n",
        "print('RMSE =', round(mean_squared_error(y_test, y_pred)**.5,2))"
      ],
      "metadata": {
        "id": "6JszMuQREFOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter a importância das características\n",
        "feature_importance = rf_reg.feature_importances_\n",
        "\n",
        "# Obter os nomes das características\n",
        "feature_names = X_Cols.columns\n",
        "\n",
        "# Criar um DataFrame para facilitar a visualização\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
        "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
        "\n",
        "# Criar um gráfico de barras para visualizar a importância das características\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance do Modelo Random Forest')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TUDVxaObGH2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import learning_curve\n",
        "train_sizes, train_scores, test_scores = learning_curve(rf_model, X_Cols, Y_Cols,\n",
        "            shuffle=False, scoring=\"neg_mean_absolute_percentage_error\", train_sizes = [1, 100, 500, 1000, 1500, 2000, 2300]\n",
        ")\n",
        "\n",
        "plt.plot(train_sizes,np.mean(train_scores,axis=1), label=\"train\")\n",
        "plt.plot(train_sizes,np.mean(test_scores,axis=1), label=\"test\")\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Training Set Size\")\n",
        "plt.ylabel(\"MAPE\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fy0onW3-Eyvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo e acompanhar a curva de perda\n",
        "# Atenção, esse processo costuma demorar alguns minutos.\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for i in range(1, n_estimators + 1):\n",
        "    rf_model.set_params(n_estimators=i)\n",
        "    rf_model.fit(X_train, y_train)\n",
        "\n",
        "    train_loss = np.mean((y_train - rf_model.predict(X_train)) ** 2)\n",
        "    test_loss = np.mean((y_test - rf_model.predict(X_test)) ** 2)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "# Plotar a curva de perda\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, n_estimators + 1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, n_estimators + 1), test_losses, label='Test Loss')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Random Forest Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "18hGB5UgE1NT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lkzVs_zf8ykr",
        "r1uMkGg149uM",
        "IMV7bPb74Eh9",
        "sk6gOkW24L73",
        "C3PzgDFQJBBy",
        "j1qJlSjD4Ydm",
        "NMZkm9Dq7l5p",
        "_-amstEY8iWJ",
        "FD4xdaY590ua",
        "yXnOOazIiMKL",
        "CBnFHBUmiZgw"
      ]
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}